{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_FL_PySyft.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaK5P2JhMv+ubkMnRRoJda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkimbf/FL-Tutorials/blob/master/MNIST_FL_PySyft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhW84UeinIlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e06b2d6d-7e78-4530-83dc-eb83d8a57878"
      },
      "source": [
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b ryffel/4P --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-encrypted\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/be/a4c0af9fdc5e5cee28495460538acf2766382bd572e01d4847abc7608dba/tf_encrypted-0.5.9-py3-none-manylinux1_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 4.4MB/s \n",
            "\u001b[?25hCollecting tensorflow<2,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/36/9a02e27f0ec248b676a380ffe910c1858e3af3027c0d4d513dd0b56a5613/tensorflow-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (1.18.5)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.9.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.12.0->tf-encrypted) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.12.0->tf-encrypted) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2,>=1.12.0->tf-encrypted) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.12.0->tf-encrypted) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.12.0->tf-encrypted) (3.1.0)\n",
            "Building wheels for collected packages: pyyaml, gast\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=37840118778b8a89618d836309518e54067ff26d43220263f5287ce38cbbc25b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=ba34658eedc4b2a7d24b8cfb4b4937d053f0bfb8b107f30a4f6dadfa9e870d0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built pyyaml gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow, pyyaml, tf-encrypted\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed gast-0.2.2 pyyaml-5.3.1 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1 tf-encrypted-0.5.9\n",
            "Cloning into 'PySyft'...\n",
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 35445 (delta 0), reused 2 (delta 0), pack-reused 35443\u001b[K\n",
            "Receiving objects: 100% (35445/35445), 36.76 MiB | 19.66 MiB/s, done.\n",
            "Resolving deltas: 100% (23880/23880), done.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "examples.tutorials.advanced.websockets_mnist.__pycache__.start_websocket_servers.cpython-36: module references __file__\n",
            "examples.tutorials.advanced.websockets_mnist_parallel.__pycache__.start_websocket_servers.cpython-36: module references __file__\n",
            "examples.tutorials.websocket.pen_testing.steal_data_over_sockets.__pycache__.start_websocket_servers.cpython-36: module references __file__\n",
            "test.__pycache__.run_websocket_server.cpython-36: module references __file__\n",
            "test.scripts.__pycache__.run_websocket_server.cpython-36: module references __file__\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Collecting lz4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/6a/ea95dd9a9957143636cfad5037637abec91016b9bde519d3edf4708e3d83/lz4-3.1.0-cp36-cp36m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.5MB/s \n",
            "\u001b[31mERROR: syft 0.2.4 has requirement lz4~=3.0.2, but you'll have lz4 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement Pillow~=6.2.2, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement requests~=2.22.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement torch~=1.4.0, but you'll have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement torchvision~=0.5.0, but you'll have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "  Found existing installation: lz4 3.0.2\n",
            "    Uninstalling lz4-3.0.2:\n",
            "      Successfully uninstalled lz4-3.0.2\n",
            "Successfully installed lz4-3.1.0\n",
            "Collecting websocket\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6d/a60d620ea575c885510c574909d2e3ed62129b121fa2df00ca1c81024c87/websocket-0.2.1.tar.gz (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 4.4MB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/8b/918c61f23693971f4eedb3f910438e18b9bab489511dd1f313657ef0a330/gevent-20.6.1-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 7.5MB/s \n",
            "\u001b[?25hCollecting greenlet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 44.1MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/95/f657b6e17f00c3f35b5f68b10e46c3a43af353d8856bd57bfcfb1dbb3e92/setuptools-47.1.1-py3-none-any.whl (583kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 44.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: websocket\n",
            "  Building wheel for websocket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websocket: filename=websocket-0.2.1-cp36-none-any.whl size=192134 sha256=b810d30cedf0bf738572ecf61659a3b481dd3cdf93237a8918bf719ae4477268\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f7/5c/9e8243838269ea93f05295708519a6e183fa6b515d9ce3b636\n",
            "Successfully built websocket\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft-proto 0.2.9a2 has requirement protobuf>=3.11.1, but you'll have protobuf 3.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, zope.event, greenlet, zope.interface, gevent, websocket\n",
            "  Found existing installation: setuptools 47.1.1\n",
            "    Uninstalling setuptools-47.1.1:\n",
            "      Successfully uninstalled setuptools-47.1.1\n",
            "Successfully installed gevent-20.6.1 greenlet-0.4.16 setuptools-47.1.1 websocket-0.2.1 zope.event-4.4 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting websockets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.0MB/s \n",
            "\u001b[31mERROR: syft 0.2.4 has requirement lz4~=3.0.2, but you'll have lz4 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement Pillow~=6.2.2, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement requests~=2.22.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement torch~=1.4.0, but you'll have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement torchvision~=0.5.0, but you'll have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets\n",
            "  Found existing installation: websockets 8.1\n",
            "    Uninstalling websockets-8.1:\n",
            "      Successfully uninstalled websockets-8.1\n",
            "Successfully installed websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k_qdh8evOOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "c8a63901-343e-489c-ba78-c9a63ce4b16b"
      },
      "source": [
        "!pip install torchvision==0.5.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 16kB/s \n",
            "\u001b[31mERROR: syft 0.2.4 has requirement lz4~=3.0.2, but you'll have lz4 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement Pillow~=6.2.2, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.2.4 has requirement requests~=2.22.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsqdPuryoXMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "c52c5246-06da-48c4-ca47-570ec12f86eb"
      },
      "source": [
        "!pip install syft_proto"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft_proto in /usr/local/lib/python3.6/dist-packages/syft_proto-0.2.9a2-py3.6.egg (0.2.9a2)\n",
            "Collecting protobuf>=3.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.1->syft_proto) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.1->syft_proto) (47.1.1)\n",
            "Installing collected packages: protobuf\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "Successfully installed protobuf-3.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_DHU3Udnc_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9afad8f7-b7e9-46b5-8c10-154796900690"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import logging\n",
        "\n",
        "# Importing PySyft to simulate federated learning\n",
        "import syft as sy\n",
        "\n",
        "# Hooking PyTorch to PySyft\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Creating two imaginary clients\n",
        "school_1 = sy.VirtualWorker(hook, id=\"westside\")\n",
        "school_2 = sy.VirtualWorker(hook, id=\"grapevine\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XES59W4PpjPL",
        "colab_type": "text"
      },
      "source": [
        "## Defining hyper-parameters: learning rate, batch size, test batch size, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzfvVNA2oLhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the args\n",
        "args = {\n",
        "    'use_cuda' : True,\n",
        "    'batch_size' : 64,\n",
        "    'test_batch_size' : 1000,\n",
        "    'lr' : 0.01,\n",
        "    'log_interval' : 10,\n",
        "    'epochs' : 10\n",
        "}\n",
        "\n",
        "# Checking to use GPU or not\n",
        "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6R9NFBepyQ1",
        "colab_type": "text"
      },
      "source": [
        "## Defining a very simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dbXio5ypd3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a simple CNN net\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*12*12, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=10)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 64*12*12)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwFcO7yHrxO7",
        "colab_type": "text"
      },
      "source": [
        "## Sending the data to schools\n",
        "```.federate()```\n",
        "* splits the dataset in two parts\n",
        "* sends this data across two remote workers (two schools)\n",
        "\n",
        "<br> * No need to send data in real life"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoRWtmZgrdnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use PySyft's API to prepare the data and distribute across 2 worker\n",
        "# This is just to simulate federated learning\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,),(0.3081,))\n",
        "                   ]))\n",
        "    .federate((school_2, school_1)),\n",
        "    batch_size=args['batch_size'], \n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# test data remains with me locally\n",
        "# normal torch code to load test data from MNIST\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=args['test_batch_size'],\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jItVZbeBw5JB",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation functions\n",
        "\n",
        "> Each time we train the model, need to send it to the right location for each batch (using ```.send()``` function)\n",
        "<br><br> We perform all the operations **REMOTELY** with the same syntax like we're doing local PyTorch \n",
        "<br><br> When we're done, get back the updated model using the ```.get()``` method.\n",
        "\n",
        "So Simple!\n",
        "\n",
        "<br>Note that ```(data, target)``` is a pair of ```PointerTensor``` in the ```train``` function below\n",
        "\n",
        "In a ```PointerTensor```, I can **get the worker** it points to using ```.location``` attribute.\n",
        "\n",
        "```.location``` attribute is what I need to **use to send the model to the correct location**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz4SsWp3ujwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    # Iterating over federated data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # Sending the model to the remote location\n",
        "        model = model.send(data.location)\n",
        "\n",
        "        # Same torch code\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        # this loss is a ptr to the tensor loss at the remote location\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # call backward() on the loss ptr,\n",
        "        # that will send the command to call backward \n",
        "        # on the actual loss tensor present on the remote machine\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get back the updated model\n",
        "        model.get()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            # note that the variable loss was also created at remote worker,\n",
        "            # so I need to explicitly get it back\n",
        "            loss = loss.get()\n",
        "\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch,\n",
        "                    batch_idx * args['batch_size'], # no of images done\n",
        "                    len(train_loader) * args['batch_size'], # total images left\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss.item()\n",
        "                )\n",
        "            )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii55vJ0p18wa",
        "colab_type": "text"
      },
      "source": [
        "The same test function since it is ran locally on my machine.\n",
        "<br> (only training happens remotely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwCqvQ0a151M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # Add losses together\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "\n",
        "            # Get the index of the max probability class\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgcGxtVl3krP",
        "colab_type": "text"
      },
      "source": [
        "## Training!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07pRKVoZ3Z1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c66ea0b6-dee6-463b-defe-81c64907e6c3"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'])\n",
        "\n",
        "logging.info(\"Starting training!!\")\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.304626\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.184304\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.012875\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.753484\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.295024\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 0.995949\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.879802\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.903575\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.601575\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.510105\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.471961\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.348593\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.298909\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.332955\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.475853\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.303741\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.319659\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.324476\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.307374\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.518553\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.641065\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.320979\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.414029\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.538288\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.232775\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.133528\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.338944\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.398112\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.342485\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.393590\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.164750\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.279929\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.396937\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.299301\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.249144\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.152566\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.259188\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.289093\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.166858\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.378415\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.170147\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.246950\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.167717\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.448229\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.409789\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.391351\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.174505\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.439827\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.203764\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.194745\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.297430\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.263907\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.206401\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.122195\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.293027\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.219264\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.306696\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.512193\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.330214\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.337536\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.291741\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.201549\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.131709\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.085335\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.234164\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.199829\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.397371\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.317059\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.145632\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.206369\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.205108\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.150720\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.318681\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.131035\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.207903\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.107783\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.438070\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.267921\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.338718\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.117977\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.046946\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.132233\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.287341\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.337338\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.247849\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.187306\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.143230\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.253404\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.200377\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.159649\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.163879\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.220511\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.178599\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.276563\n",
            "\n",
            "Test set: Average loss: 0.1796, Accuracy: 9465/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.160400\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.085986\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.170849\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.192802\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.121026\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.143849\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.217463\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.073987\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.223153\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.154888\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.227560\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.224098\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.219412\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.289629\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.234297\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.428236\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.187143\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.319327\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.137621\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.163028\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.103911\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.084142\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.128194\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.324485\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.172968\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.224962\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.112731\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.078723\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.418933\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.161043\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.086451\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.194670\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.245107\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.199035\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.205552\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.170249\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.076827\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.250660\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.146913\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.209882\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.201088\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.109930\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.169914\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.111127\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.138787\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.082678\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.290994\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.076250\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.145225\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.198261\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.108695\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.107566\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.327124\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.185294\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.283816\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.112858\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.163864\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.275798\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.263760\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.308459\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.340629\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.158424\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.105443\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.137608\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.120782\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.086457\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.092670\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.117997\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.093218\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.159476\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.107713\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.066231\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.158550\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.073315\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.190586\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.080221\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.091741\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.118984\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.198724\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.051862\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.147106\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.084332\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.065448\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.367556\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.210814\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.093255\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.217251\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.182654\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.100838\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.163739\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.085288\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.354172\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.185732\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.068858\n",
            "\n",
            "Test set: Average loss: 0.1176, Accuracy: 9652/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.130135\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.064120\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.128766\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.109515\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.187782\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.056183\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.074779\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.273959\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.154251\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.020605\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.140471\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.121314\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.236150\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.120777\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.112086\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.239425\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.225275\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.162306\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.093695\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.093531\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.170637\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.141529\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.118627\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.038043\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.128311\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.047236\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.120188\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.042415\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.047564\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.093630\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.093527\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.064467\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.095781\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.130414\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.067007\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.190700\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.097458\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.061522\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.086755\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.099671\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.063457\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.106433\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.108254\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.094903\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.085089\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.126598\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.138313\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.156633\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.089933\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.069535\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.073206\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.207541\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.157606\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.089605\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.060915\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.030582\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.052182\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.071697\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.281315\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.066431\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.113398\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.118456\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.047146\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.099257\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.134441\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.106384\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.064273\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.060900\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.012236\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.053348\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.141158\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.066414\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.054939\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.068034\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.088726\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.079824\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.167453\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.036782\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.067061\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.233421\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.041162\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.169805\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.172227\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.098868\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.091175\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.093687\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.204647\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.133825\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.069690\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.086549\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.052141\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.189455\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.088793\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.140220\n",
            "\n",
            "Test set: Average loss: 0.0944, Accuracy: 9702/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.143303\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.072293\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.090084\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.118324\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.040254\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.073510\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.074994\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.076800\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.146783\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.095175\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.083668\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.067075\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.073854\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.056665\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.058029\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.041845\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.084295\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.117841\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.207057\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.062704\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.020144\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.086267\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.038842\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.092644\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.035958\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.088061\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.066246\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.095256\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.053808\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.090261\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.140814\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.097612\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.046036\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.090842\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.035359\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.066371\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.073179\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.101101\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.125304\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.077143\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.073730\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.120875\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.020179\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.040692\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.037184\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.068817\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.176466\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.052124\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.073140\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.072485\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.027989\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.095439\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.041166\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.034775\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.093621\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.127119\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.094855\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.139487\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.058304\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.230409\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.091411\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.030993\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.035901\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.050709\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.024466\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.093841\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.076684\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.119963\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.031744\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.055284\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.023706\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.199331\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.050311\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.009877\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.016944\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.053289\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.049540\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.169291\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.086775\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.038767\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.031054\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.029307\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.139855\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.018816\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.031990\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.055366\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.071913\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.125517\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.051487\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.114627\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.027832\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.085001\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.081867\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.023418\n",
            "\n",
            "Test set: Average loss: 0.0778, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.038215\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.140832\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.054752\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.153837\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.050748\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.062474\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.028324\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.044650\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.042795\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.089441\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.031963\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.105818\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.036186\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.074568\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.084145\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.146866\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.019453\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.064492\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.098554\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.042312\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.035793\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.015562\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.104745\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.021656\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.097905\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.021708\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.018035\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.075201\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.134803\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.019588\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.019497\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.056089\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.043797\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.026282\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.077242\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.217807\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.080996\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.053782\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.034718\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.007069\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.039018\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.049818\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.054311\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.024076\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.012089\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.081010\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.069429\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.073173\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.080079\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.103758\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.042220\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.101066\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.009982\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.022686\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.065721\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.029428\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.062992\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.080095\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.025049\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.124505\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.025792\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.030541\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.021171\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.076396\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.074356\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.087080\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.073201\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.042556\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.142149\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.006636\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.086615\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.032772\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.045718\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.135371\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.046680\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.037654\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.039782\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.095471\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.044045\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.115232\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.244916\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.256488\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.076380\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.024911\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.051977\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.030998\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.029060\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.031371\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.043818\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.006377\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.056108\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.163085\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.045979\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.104915\n",
            "\n",
            "Test set: Average loss: 0.0627, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.017585\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.015689\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.115966\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.048363\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.115541\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.056441\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.048751\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.082602\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.062208\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.109966\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.080714\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.071917\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.026954\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.022528\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.207806\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.065372\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.036241\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.024267\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.019577\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.016948\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.054492\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.053899\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.106225\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.276585\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.061407\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.023380\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.048534\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.023822\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.060609\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.027017\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.121688\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.018778\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.045971\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.031057\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.091043\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.014502\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.094536\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.011551\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.045340\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.007663\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.051471\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.168134\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.039626\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.048096\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.027714\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.016682\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.061855\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.004814\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.030029\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.021925\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.053901\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.142782\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.016367\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.039333\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.024618\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.114757\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.034427\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.043202\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.115499\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.025403\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.023153\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.015850\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.058920\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.015016\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.061307\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.037454\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.132598\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.022110\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.036413\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.023207\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.148595\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.068760\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.162400\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.103906\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.019474\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.217108\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.026528\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.108220\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.083208\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.066842\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.019479\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.024030\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.122507\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.083265\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.029272\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.038749\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.086847\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.055554\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.059151\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.070480\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.094990\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.134450\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.024357\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.033220\n",
            "\n",
            "Test set: Average loss: 0.0582, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.057312\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.048647\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.010522\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.021659\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.167735\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.021493\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.044598\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.140632\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.078141\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.025542\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.013634\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.010309\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.113902\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.072504\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.043062\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.048158\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.021483\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.014979\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.017939\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.029250\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.031854\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.039941\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.020877\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.023376\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.050555\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.041516\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.019389\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.009481\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.037753\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.080748\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.041379\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.091113\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.047939\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.023061\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.060057\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.033307\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.031239\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.008697\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.004887\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.108659\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.046385\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.032336\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.063933\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.017547\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.010538\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.038046\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.034866\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.035797\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.070380\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.020548\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.067656\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.023462\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.063951\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.016282\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.011562\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.057156\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.014661\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.065888\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.056317\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.060204\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.099870\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.029485\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.022773\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.088509\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.064173\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.019582\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.020663\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.015106\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.197789\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.198437\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.012086\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.067637\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.057902\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.012251\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.068116\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.027782\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.018691\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.009641\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.137826\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.029414\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.014180\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.029719\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.029286\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.045115\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.010040\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.029236\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.022586\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.043509\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.117528\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.029126\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.063263\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.097281\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.126152\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.066791\n",
            "\n",
            "Test set: Average loss: 0.0451, Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.031712\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.025342\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.040897\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.045587\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.041943\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.018506\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.040046\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.011528\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.047997\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.027672\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.072435\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.071936\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.012963\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.013008\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.009061\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.013652\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.003587\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.063484\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.078447\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.019090\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.004824\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.048573\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.013211\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.016033\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.005181\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.020335\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.005586\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.122892\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.048059\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.081751\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.065092\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.064420\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.017727\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.036550\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.037106\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.052135\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.017993\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.067155\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.016256\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.054137\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.032148\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.022669\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.049123\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.053075\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.082394\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.024628\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.053760\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.011408\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.016634\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.021185\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.023625\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.036197\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.012648\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.027468\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.026027\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.024595\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.022639\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.102869\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.038063\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.038634\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.017801\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.077052\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.032228\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.019309\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.020818\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.038408\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.087915\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.019496\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.017457\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.029466\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.003083\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.158204\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.029747\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.135576\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.071084\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.013361\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.037577\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.051617\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.006633\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.115376\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.054286\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.021293\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.141264\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.020838\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.058089\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.006259\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.037093\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.035136\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.009427\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.015326\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.032865\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.035489\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.039553\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.021614\n",
            "\n",
            "Test set: Average loss: 0.0549, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.027211\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.083573\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.043899\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.016685\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.083232\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.028012\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.015696\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.024908\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.022402\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.020285\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.033269\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.035291\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.005894\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.014632\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.098958\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.013952\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.022246\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.035731\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.027957\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.007113\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.007222\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.040464\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.025068\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.050776\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.018636\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.054613\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.031049\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.027038\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.019621\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.021006\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.012978\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.074866\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.091282\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.010154\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.006376\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.003255\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.009240\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.033136\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.142533\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.034956\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.042777\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.002433\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.086442\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.094826\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.050666\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.005798\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.016201\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.021943\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.089379\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.013730\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.060748\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.004295\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.009308\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.025595\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.009931\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.011805\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.004374\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.032795\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.031075\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.037077\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.129816\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.007959\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.031441\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.164068\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.011793\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.018755\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.012260\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.035370\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.029445\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.078212\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.014544\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.026804\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.015983\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.023021\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.151518\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.006459\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.047959\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.046902\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.025726\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.028260\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.057141\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.040254\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.128605\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.011577\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.006323\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.023603\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.004348\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.009555\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.011531\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.006757\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.018920\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.045380\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.139821\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.016350\n",
            "\n",
            "Test set: Average loss: 0.0442, Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.012009\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.014859\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.018110\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.017566\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.020515\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.013453\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.002412\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.009548\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.005220\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.035091\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.021155\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.005353\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.026779\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.061553\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.016641\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.021822\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.088241\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.030064\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.015543\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.086897\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.031061\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.045724\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.050366\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.015921\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.117650\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.013046\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.022696\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.100793\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.094555\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.038216\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.043982\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.024398\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.159003\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.015387\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.041904\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.004823\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.003959\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.009744\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.019041\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.011689\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.015243\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.025615\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.004751\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.014115\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.017447\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.036858\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.004769\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.010033\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.005896\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.006141\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.054259\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.004648\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.023084\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.034016\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.074752\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.048003\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.006010\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.074852\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.102469\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.011066\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.044479\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.023107\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.082995\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.001458\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.032281\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.109385\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.049093\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.029232\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.024180\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.006542\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.025078\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.025349\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.017422\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.017634\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.021734\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.009315\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.036825\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.002849\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.006733\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.010505\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.091524\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.013268\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.025366\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.046098\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.018130\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.043365\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.007976\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.014871\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.022687\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.006859\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.105169\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.021261\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.008254\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.009810\n",
            "\n",
            "Test set: Average loss: 0.0416, Accuracy: 9859/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imc_2uPS4Orv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}