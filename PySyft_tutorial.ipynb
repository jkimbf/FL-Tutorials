{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySyft_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxnf2Xyml+PhmExcDnMHCv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkimbf/FL-Tutorials/blob/master/PySyft_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvJ5FfZqE5SS",
        "colab_type": "text"
      },
      "source": [
        "# PySyft\n",
        "**PySyft** is a wrapper around PyTorch and adds extra functionality to it to facilitate federated learning.\n",
        "\n",
        "<br> Installation: ```pip install syft``` ([Installation guide](https://github.com/OpenMined/PySyft/blob/master/INSTALLATION.md))\n",
        "\n",
        "<br> How do we train a model on data we don't have access to or cannot see?\n",
        "> We want to perform deep learning on some **remote machines**. <br> Thus, instead of using Torch ```tensor```, we are now going to work with a **```pointer``` to ```tensor```** <br> (this ```tensor``` is stored on the remote location) with PySyft.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE3yeNcbHzck",
        "colab_type": "text"
      },
      "source": [
        "## I. Imports and Hook\n",
        "\n",
        "```\n",
        "import torch\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n",
        "```\n",
        "```TorchHook```: wrapping by \"adding all the additional functionality\" to Torch for Federated Learning and other Private AI techniques.\n",
        "\n",
        "<br> **Hook** overrides mothods on PyTorch Tensors.\n",
        "> to extend torch methods to allow for moving of tensors from one worker to another <br> to execute commands on one worker that are called on tensors controlled by the local worker\n",
        "\n",
        "<br> **FIRST THING** I will initialize when using PySyft with PyTorch <br> --> **Augmenting PyTorch** with PySyft's added functionality (e.g. remote execution)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvwGog-DP2fP",
        "colab_type": "text"
      },
      "source": [
        "## II. Creating a Virtual Worker\n",
        "\n",
        "```\n",
        "# Create a Machine owned by harmony clinic\n",
        "harmony_clinic = sy.VirtualWorker(hook=hook, id = 'clinic')\n",
        "```\n",
        "What does it mean by a ```pointer``` to ```tensor```\n",
        "> Let's create a virtual machine first (e.g. owned by some health clinic, say ```harmony_clinic```) <br> Use this to simulate a machine present at a remote location <br>Note: ```syft``` calls these machines as ```VirtualWorker```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBmZqW8KTLUt",
        "colab_type": "text"
      },
      "source": [
        "## III. Sending the Tensors\n",
        "Now I know that the ```harmony_clinic``` is at a random location.\n",
        "\n",
        "Now, let's create some data and send it to ```harmony_clinic```\n",
        "```\n",
        "# Create a Tensor (e.g. some gene sequence)\n",
        "dna = torch.tensor([0,1,2,1,2])\n",
        "\n",
        "# Send it, and in turn get a pointer back that points to that Tensor\n",
        "dna_ptr = dna.send(harmony_clinic)\n",
        "\n",
        "print(dna_ptr)\n",
        "```\n",
        "```PointerTensor``` points from ```me``` to ```harmony_clinic``` (random numbers are the object IDs assigned by PySyft)\n",
        "\n",
        "<br> Now, ```harmony_clinic``` has the **tensor**.\n",
        "\n",
        "```harmony_clinic._objects``` shows the objects that ```harmony_clinic``` currently has\n",
        "<br>\n",
        "```\n",
        "print(harmony_clinic._objects)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JMTt6i9V-EP",
        "colab_type": "text"
      },
      "source": [
        "## IV. Getting back the Tensors\n",
        "```.get()```: **take the ```tensor``` back** from a remote location\n",
        "```\n",
        "# get back dna\n",
        "dna = dna_ptr.get()\n",
        "print(dna)    # tensor({0, 1, 2, 1, 2})\n",
        "\n",
        "# clinic no longer has tensor dna, moved back to out machine\n",
        "print(harmony_clinic._objects())    # {}\n",
        "```\n",
        "\n",
        "I can ```use pointers to do arithmetic``` as in PyTorch\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbHbDFU3XMmD",
        "colab_type": "text"
      },
      "source": [
        "## V. Doing Deep Learning with Pointer Tensors\n",
        "```\n",
        "a = torch.tensor([3.14, 6.28]).send(harmonic_clinic)\n",
        "b = torch.tensor([6.14, 3.28]).send(harmonic_clinic)\n",
        "c = a + b\n",
        "print(c)\n",
        "```\n",
        "When ```c = a + b```, a command is sent to the **\"\"\"remote machine\"\"\"** to **\"\"\"do exact calculation\"\"\"**\n",
        "> Creates a ```new tensor``` on its machine <br> send back a ```pointer to tensor c```\n",
        "\n",
        "This API had been extended to all the PyTorch operations including Back propagation\n",
        "\n",
        "Meaning that, \n",
        "<br>```We can use the same PyTorch code that we usually do when doing Maching Learning```\n",
        "\n",
        "```\n",
        "# Create two tensors and send it\n",
        "train = torch.tensor([2.4, 6.2], requires_grad=True).send(harmony_clinic)\n",
        "label = torch.tensor([2, 6.]).send(harmony_clinic)\n",
        "\n",
        "# Apply some function\n",
        "# Since this is just to show the idea, use rather simple one, L1 loss\n",
        "loss = (train-label).abs().sum()\n",
        "\n",
        "# Even .backward() works with pointers\n",
        "loss.backward()\n",
        "\n",
        "# Retrieve back the train tensor\n",
        "train = train.get()\n",
        "\n",
        "print(train)    # tensor([2.4, 6.2], requires_grad=True)\n",
        "\n",
        "# If everything went well, \"graditents accumulated\" should be in .grad attribute of train\n",
        "print(train.grad)    # tensor([1., 1.])\n",
        "```\n",
        "\n",
        "the API is quite flexible and capable of performing nearly any operation I would normally perform in Torch on remote data\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}